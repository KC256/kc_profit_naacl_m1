{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder_path  /home/fukuda/profit-naacl/profit-naacl/test_data_sorted_in_jsons_6\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "# from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "# import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# import numpy as np\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "num_stocks = 20\n",
    "lookback_length = 7\n",
    "num_texts_per_day = 5 #ツイートが３０以上ある場合その箇所は省かれる\n",
    "embedding_size = 384\n",
    "\n",
    "folder_path = \"/home/fukuda/profit-naacl/profit-naacl/test_data_sorted_in_jsons_6\" #pickleファイル作成の対象となるフォルダ\n",
    "print(\"folder_path \", folder_path)\n",
    "parts = folder_path.split('/')\n",
    "folder_name = parts[-1]\n",
    "\n",
    "stock_price_folder = \"/home/fukuda/stocknet-dataset/price/raw\"\n",
    "\n",
    "text_difficulty = torch.ones(num_stocks)\n",
    "volatility = torch.ones(num_stocks)\n",
    "price_text_difficulty = torch.ones(num_stocks)\n",
    "price_text_vol_difficulty = torch.ones(num_stocks)\n",
    "price_difficulty = torch.ones(num_stocks)\n",
    "#ここまで初期設定\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "FLOAT = torch.cuda.FloatTensor if USE_CUDA else torch.FloatTensor\n",
    "def to_tensor(ndarray, volatile=False, requires_grad=False, dtype=FLOAT):\n",
    "    return Variable(\n",
    "        torch.from_numpy(ndarray), volatile=volatile, requires_grad=requires_grad\n",
    "    ).type(dtype)\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "#token(1ツイート)に対し７６８ベクトルを返す\n",
    "def get_average_embedding_output(tokens):\n",
    "    result = \" \".join(tokens)\n",
    "    embeddings = model.encode(result)\n",
    "    embeddings = embeddings.tolist()\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "#file_pathのjson_length, average_output, time_differenceを返す\n",
    "def get_embedding_length_timefeatures(file_path, num_texts_per_day):\n",
    "\n",
    "    # ll_json_length_array = np.zeros((len(stock_names), date_length))\n",
    "    # average_output = np.empty(num_texts_per_day)\n",
    "    # time_difference = np.zeros(num_texts_per_day)\n",
    "\n",
    "    # average_output = [0]*num_texts_per_day #ツイートが一日３０件未満の場合その箇所の値は０\n",
    "    embedding_size=384\n",
    "    average_output = [[0 for _ in range(embedding_size)] for _ in range(num_texts_per_day)]\n",
    "    # print(len(average_output[1]))\n",
    "    time_difference = [[0]]*num_texts_per_day #ツイートが一日３０件未満の場合その箇所の値は０\n",
    "    json_length = 0\n",
    "    try:\n",
    "        with open(file_path, 'r') as json_file:\n",
    "            # print(\"json_file\", json_file)\n",
    "            for line in json_file:\n",
    "                if(json_length<num_texts_per_day): #ツイートが1日num_text_per_day件以上の場合num_text_per_day+1件目以降はパス\n",
    "                    data_dict = json.loads(line) \n",
    "                    print(\"data_dict[text]\", data_dict[\"text\"])\n",
    "                    tokens = data_dict[\"text\"]\n",
    "                    average_output[json_length] = get_average_embedding_output(tokens)\n",
    "                    # print(average_output)\n",
    "\n",
    "                    # print(data_dict[\"created_at\"])\n",
    "                    now_datetime_obj = datetime.strptime(data_dict[\"created_at\"], \"%a %b %d %H:%M:%S %z %Y\")\n",
    "                    if(json_length>0):\n",
    "                        time_difference_object = now_datetime_obj - past_datetime_obj\n",
    "                        time_difference[json_length] = [int(time_difference_object.total_seconds())]\n",
    "                        # print(f\"時間差: {time_difference}\")\n",
    "                    else:#第一要素の場合時間差０とする\n",
    "                        time_difference[json_length] = [0]\n",
    "                    past_datetime_obj = now_datetime_obj\n",
    "                    json_length += 1\n",
    "    except FileNotFoundError:\n",
    "        print(file_path, \"does not exist\")\n",
    "        pass\n",
    "    \n",
    "    # average_output_tensor = to_tensor(average_output)\n",
    "    return json_length, average_output, time_difference\n",
    "    \n",
    "# json_length, average_output, time_difference = get_embedding_length_timefeatures(\"/home/fukuda/profit-naacl/profit-naacl/test_data_sorted_2/AMZN/2014-01-02\", 30)\n",
    "# print(time_difference)\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "#stockname date_targetからそれに対応する日付の企業のadj_closeを得る　ソースは/home/fukuda/AI4Finance-Foundation/stocknet-dataset/price/preprocessed\n",
    "def get_adj_close(stock_name, date_target, date_target_object, stock_price_folder):\n",
    "    stock_file = stock_price_folder + \"/\" + stock_name + \".csv\"\n",
    "    df = pd.read_csv(stock_file)\n",
    "    \n",
    "    target_data = pd.DataFrame()\n",
    "    while(target_data.empty): #その日付が存在するまで\n",
    "        # print(date_target)\n",
    "        target_data = df[df[\"Date\"] == date_target]\n",
    "        date_target_object -= timedelta(days=1) #その日の終値が存在しない場合1日前の終値を代入する　一日前も存在しない場合は二日前。。\n",
    "        date_target = date_target_object.strftime(\"%Y-%m-%d\")\n",
    "    adj_close = target_data[\"Adj Close\"].iloc[0]\n",
    "\n",
    "    return adj_close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pickle(date_start, date_end):\n",
    "    date_start_object= datetime.strptime(date_start, \"%Y-%m-%d\")\n",
    "    date_end_object= datetime.strptime(date_end, \"%Y-%m-%d\")\n",
    "    date_before_start_object = date_start_object - timedelta(days=lookback_length) #date_start からlookbacklength前の日\n",
    "    date_before_start = date_before_start_object.strftime(\"%Y-%m-%d\")\n",
    "    date_target = date_before_start\n",
    "    date_target_object = date_before_start_object\n",
    "    stock_names = sorted(os.listdir(folder_path))\n",
    "    date_length_object = date_end_object - date_before_start_object + timedelta(days=1)\n",
    "    date_length = date_length_object.days\n",
    "    # print(date_length, type(date_length))\n",
    "    # sys.exit()\n",
    "\n",
    "    # all_embedding_array = np.empty((len(stock_names), date_length), dtype=object)\n",
    "    all_json_length_array = np.zeros((len(stock_names), date_length))\n",
    "    # all_time_difference_array = np.empty((len(stock_names), date_length))\n",
    "\n",
    "    # embedding_array = np.empty((len(stock_names), lookback_length), dtype=object)\n",
    "    json_length_array = np.zeros((len(stock_names), lookback_length))\n",
    "    # time_difference_array = np.empty((len(stock_names), lookback_length))\n",
    "    # print(type(time_difference_array))\n",
    "\n",
    "    all_time_difference_list = [[0] * date_length for _ in range(len(stock_names))]\n",
    "    time_difference_list = [[None] * lookback_length for _ in range(len(stock_names))]\n",
    "    # print(all_time_difference_list)\n",
    "    # all_embedding_list = [[None] * date_length for _ in range(len(stock_names))]\n",
    "    # embedding_list = [[None] * lookback_length for _ in range(len(stock_names))]\n",
    "\n",
    "    all_embedding_list = [[[[0 for _ in range(embedding_size)] for _ in range(num_texts_per_day)] for _ in range(date_length)] for _ in range(num_stocks)]\n",
    "    embedding_list =[[[[0 for _ in range(embedding_size)] for _ in range(num_texts_per_day)] for _ in range(lookback_length)] for _ in range(num_stocks)]\n",
    "\n",
    "\n",
    "\n",
    "    #date_targetが最終日になるまで 各日の処理\n",
    "    preprosessed_data = [None] * (date_length-lookback_length)\n",
    "    j = 0 #日にちカウント\n",
    "    k = [0]*len(stock_names) #all_embedding_list等カウント　日付詰めるため\n",
    "    print(\"stock_names\", stock_names)\n",
    "    print(\"date_target_object\", date_target_object)\n",
    "    # sys.exit()\n",
    "    while(date_target_object != date_end_object + timedelta(days=1)):\n",
    "        #dates, last_dateを出す\n",
    "        date_last_object = date_target_object - timedelta(days=1)\n",
    "        date_last = date_last_object.strftime(\"%Y-%m-%d\")\n",
    "        dates_object = []\n",
    "        dates = []\n",
    "        i = lookback_length\n",
    "        while(i>0):\n",
    "            dates_object.append(date_target_object- timedelta(days=i))\n",
    "            temp = date_target_object- timedelta(days=i-1)\n",
    "            dates.append(temp.strftime(\"%Y-%m-%d\"))\n",
    "            i -= 1\n",
    "\n",
    "        adj_close_target = [None] * len(stock_names)\n",
    "        adj_close_last = [None] * len(stock_names)\n",
    "        i = 0\n",
    "        #各企業に対する処理\n",
    "        for stock_name in stock_names:\n",
    "            # print(i)\n",
    "            file_path =os.path.join(folder_path, stock_name, date_target) #01-02\n",
    "            print(file_path)\n",
    "            json_length, average_output, time_difference= get_embedding_length_timefeatures(file_path, num_texts_per_day) #これは当日のデータ\n",
    "            adj_close_target[i]= get_adj_close(stock_name, date_target, date_target_object, stock_price_folder)\n",
    "            adj_close_last[i] = get_adj_close(stock_name, date_last, date_last_object, stock_price_folder)\n",
    "            # print(date_target, date_last, dates)\n",
    "            # print(json_length, time_difference, average_output)\n",
    "            # print(adj_close_last, adj_close_target)\n",
    "            \n",
    "            if os.path.exists(file_path) or j-lookback_length+1<=0: #[0]でない、または開始日前７日間のデータのみが入る\n",
    "                all_embedding_list[i][k[i]] = average_output\n",
    "                all_json_length_array[i][k[i]] = json_length\n",
    "                all_time_difference_list[i][k[i]] = time_difference\n",
    "                k[i] += 1\n",
    "            \n",
    "            if(j-lookback_length+1>0):\n",
    "                embedding_list[i] = all_embedding_list[i][k[i]-lookback_length:k[i]]\n",
    "                json_length_array[i] = all_json_length_array[i][k[i]-lookback_length:k[i]]\n",
    "                time_difference_list[i] = all_time_difference_list[i][k[i]-lookback_length:k[i]]\n",
    "\n",
    "            # print(\"len(embedding_list[i]) \", len(embedding_list[i]))\n",
    "            i += 1\n",
    "        \n",
    "        if(j-lookback_length+1>0):\n",
    "            # embedding_tensor = to_tensor(embedding_array)\n",
    "            json_length_tensor = to_tensor(json_length_array)\n",
    "            # print(type(json_length_tensor))\n",
    "            # print(time_difference_list)\n",
    "            # print(embedding_list)\n",
    "            # print(type(torch.tensor(time_difference_list)), type(torch.tensor(embedding_list)))\n",
    "            embedding_tensor = torch.tensor(embedding_list)\n",
    "            time_difference_tensor = torch.tensor(time_difference_list)\n",
    "            # time_difference_tensor = tf.convert_to_tensor(time_difference_array, dtype=tf.float32)\n",
    "            adj_close_target_tensor = torch.tensor(adj_close_target)\n",
    "            adj_close_last_tensor = torch.tensor(adj_close_target)\n",
    "            # print(\"date target \", date_target)\n",
    "            preprosessed_data[j-lookback_length] = {\"dates\":dates, \"date_target\":date_target, \"date_last\":date_last\n",
    "                                        , \"embedding\":embedding_tensor, \"length_data\":json_length_tensor, \"time_features\":time_difference_tensor    \n",
    "                                        , \"adj_close_last\":adj_close_last_tensor, \"adj_close_target\":adj_close_target_tensor, \"text_difficulty\":text_difficulty\n",
    "                                        , \"volatility\":volatility, \"price_text_difficulty\":price_text_difficulty, \"price_text_vol_difficulty\":price_text_vol_difficulty\n",
    "                                        , \"price_difficulty\":price_difficulty}\n",
    "\n",
    "        date_target_object += timedelta(days=1)\n",
    "        date_target = date_target_object.strftime(\"%Y-%m-%d\")\n",
    "        j += 1\n",
    "        print(k)\n",
    "        sys.exit()\n",
    "    \n",
    "    pickle_title = \"S_\" + folder_name  + \"_\" + date_start + \"~\" + date_end + \"_\" + str(num_stocks) + \"_\" + str(lookback_length)  + \"_\" + str(num_texts_per_day)  + \"_\" + str(embedding_size) + \".pkl\"\n",
    "    save_dir = \"/home/fukuda/profit-naacl/profit-naacl/pickles_0tumeru\"\n",
    "    save_path = save_dir + \"/\" + pickle_title\n",
    "\n",
    "    print(pickle_title)\n",
    "\n",
    "    # with open(save_path, \"wb\") as file:\n",
    "    #     pickle.dump(preprosessed_data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_starts = [\"2014-09-01\"]\n",
    "date_ends = [\"2014-09-02\"]\n",
    "print(\"start\")\n",
    "for date_start, date_end in zip(date_starts, date_ends):\n",
    "    print(\"Start Date:\", date_start)\n",
    "    print(\"End Date:\", date_end)\n",
    "    try:\n",
    "        make_pickle(date_start, date_end)\n",
    "    except Exception as e:\n",
    "        print(\"エラーが発生しました:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fukuda/miniconda3/envs/Profit-naacl/lib/python3.9/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "2025-02-07 17:07:45.640645: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-07 17:07:47.317455: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(384,)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "text =\"hoge\"\n",
    "embeddings = model.encode(text)\n",
    "print(embeddings.shape)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "texts = [\"hoge\", \"hage\"]\n",
    "embeddings = torch.empty(0)\n",
    "i=0\n",
    "for text in texts:\n",
    "    # embeddings[i] = model.encode(text)\n",
    "    embeddings[i] = torch.tensor(model.encode(text), dtype=torch.float32)\n",
    "    i += 1\n",
    "    \n",
    "similarity = F.cosine_similarity(embeddings, dim=0)\n",
    "print(\"Cosine Simißlarity:\", similarity.item()) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Profit-naacl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
