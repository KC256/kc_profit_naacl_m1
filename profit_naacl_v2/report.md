## `profit_naacl_v2` ディレクトリ コード解析レポート

このレポートは `kc_profit_naacl_m1/profit_naacl_v2` ディレクトリ内に含まれるPythonファイルの機能と役割をまとめたものです。

### 概要

このプロジェクトは、深層強化学習（DDPGアルゴリズム）を用いて、株式取引のポートフォリオ管理を行うためのものです。テキスト情報（ツイートやSECファイリング）を考慮に入れた取引戦略を学習し、そのパフォーマンスを評価することを目的としています。

### ファイル別解析

#### `main.py`

- **役割:** プロジェクト全体のエントリーポイントです。学習（train）と評価（test）の2つのモードを持ち、コマンドライン引数に応じて処理を切り替えます。
- **主要な処理:**
    - コマンドライン引数の解析（学習モード、使用するデータ、ハイパーパラメータなど）。
    - 学習データとテストデータをPickleファイルから読み込み。
    - `StockEnvTrade`（取引環境）、`DDPG`（エージェント）、`Evaluator`（評価器）のインスタンスを生成。
    - **学習モード (`train`):**
        - 指定されたイテレーション数だけ学習ループを実行します。
        - `agent.select_action()` で行動を選択し、`env_train.step()` で環境を1ステップ進めます。
        - `agent.observe()` で経験（状態、行動、報酬、次の状態）をリプレイバッファに保存します。
        - `agent.update_policy()` でDDPGアルゴリズムに基づき、Actor-Criticモデルを更新します。
        - 一定のステップごとに `evaluate()` を呼び出し、テストデータで現在のモデルのパフォーマンスを評価し、最も性能の良いモデルを保存します。
    - **テストモード (`test`):**
        - 保存された学習済みモデルを読み込みます。
        - `evaluate()` を呼び出し、テストデータに対するパフォーマンス（報酬、シャープレシオなど）を計算・表示します。

#### `env.py`

- **役割:** OpenAI Gymの`Env`クラスを継承した、株式取引のシミュレーション環境 `StockEnvTrade` を定義します。
- **主要な処理:**
    - **`__init__`:**
        - データセット、初期資金、ハイパーパラメータなどで環境を初期化します。
        - 状態空間（`observation_space`）と行動空間（`action_space`）を定義します。状態には、口座残高、各銘柄の株価、保有株数、テキスト特徴量（ツイート埋め込み、SECファイリング埋め込み）、時間特徴量などが含まれます。
    - **`step(actions)`:**
        - エージェントが選択した行動（`actions`）に基づき、環境を1日進めます。
        - 行動は各銘柄に対する売買の量（-1から1の範囲）で表され、これを実際の取引株数に変換します。
        - `_sell_stock()` と `_buy_stock()` を呼び出し、株の売買を実行し、口座残高と保有株数を更新します。
        - 1日の取引が終了した後、ポートフォリオの総資産価値を計算し、報酬を算出します。報酬の計算方法は複数あり、`args.diff` によって切り替え可能です（例: `price`、`vol`、`jiang`など）。
        - 次の日の状態、報酬、エピソード終了フラグ（`terminal`）などを返します。
    - **`reset()`:**
        - 環境を初期状態にリセットします。エピソードの開始時に呼び出されます。
    - **`_sell_stock()` / `_buy_stock()`:**
        - 個別の売買処理を実行する内部メソッドです。取引手数料も考慮されます。

#### `ddpg.py`

- **役割:** DDPG (Deep Deterministic Policy Gradient) アルゴリズムを実装したエージェントを定義します。
- **主要な処理:**
    - **`__init__`:**
        - ActorネットワークとCriticネットワーク、およびそれぞれのターゲットネットワークを生成します。使用するモデルは `USING_MODEL` の設定に応じて `model.py`、`model_2.py` などから動的にインポートされます。
        - Adamオプティマイザ、リプレイバッファ（`SequentialMemory`）、ノイズ生成プロセス（`OrnsteinUhlenbeckProcess`）を初期化します。
    - **`update_policy()`:**
        - リプレイバッファからミニバッチをサンプリングします。
        - Criticネットワークを更新します。ターゲットネットワークを用いて計算したQ値のターゲット（`target_q_batch`）と現在のCriticネットワークが出力するQ値との間のMSE（平均二乗誤差）を最小化するように学習します。
        - Actorネットワークを更新します。Criticが出力するQ値を最大化するような行動を出力するように学習します（方策勾配法）。
        - ターゲットネットワークをソフトアップデートします。
    - **`select_action(s_t, decay_epsilon=True)`:**
        - 現在の状態 `s_t` をActorネットワークに入力し、決定論的な行動を出力します。
        - 学習中は、探索を促進するためにOrnstein-Uhlenbeckプロセスによって生成されたノイズを付加します。`SELECT_ACTION` の設定によって、ノイズの加え方を変更できます。
    - **`observe()`:**
        - `main.py` から渡された経験（遷移）をリプレイバッファに保存します。
    - **`save_model()` / `load_weights()`:**
        - モデルの重みを保存・読み込みします。

#### `model.py`, `model_2.py`, `model_3.py`, `model_4.py`, `model_4past.py`

- **役割:** DDPGエージェントが使用するActorネットワークとCriticネットワークのアーキテクチャを定義します。ファイルごとに異なるモデル構造が実装されています。
- **共通のアーキテクチャ:**
    - **TimeLSTM:** 時間差を考慮したLSTM。ツイート間の時間的な間隔をモデルに組み込むために使用されます。
    - **Attention (`attn`)**: 特定の情報に焦点を当てるためのアテンション機構。例えば、多数のツイートの中から重要なものに重み付けをします。
    - **Actor/Criticクラス:**
        - **入力:** 状態（口座残高、株価、保有株数、テキスト特徴量など）。Criticは加えて行動も入力とします。
        - **処理:**
            1.  各銘柄、各日付のツイート埋め込みを`TimeLSTM`と`attn`で処理し、日ごとのテキスト特徴量ベクトルを生成します。
            2.  日ごとのテキスト特徴量をさらに`LSTM`と`attn`で集約し、最終的なテキスト特徴量ベクトルを生成します。
            3.  SECファイリングの埋め込みも同様に`LSTM`や`attn`で処理されます（モデルによる）。
            4.  `model_2.py`以降では、ツイート特徴量とSEC特徴量をさらにアテンション機構（`attn_x`）で統合する処理が追加されています。
            5.  テキスト特徴量と、株価や口座残高などの数値特徴量を結合し、全結合層（`Linear`）を通して最終的な出力（Actorは行動、CriticはQ値）を生成します。
- **モデル間の差異（推測）:**
    - **`model.py`:** 基本的なツイート処理モデル。
    - **`model_2.py`:** ツイートとSECファイリングの両方を入力とし、アテンションで統合するモデル。
    - **`model_3.py`:** SECファイリングのみ、または時間特徴量を付加したSECファイリングを主に使用するモデル。
    - **`model_4.py`:** ツイートとSECファイリングの両方の特徴量を結合して最終的な出力とするモデル。
    - **`model_4past.py`:** `model_4.py`の過去バージョンまたは実験的なバージョンである可能性が高いです。

#### `evaluator.py`

- **役割:** 学習済みモデルのパフォーマンスを評価するためのクラス `Evaluator` を定義します。
- **主要な処理:**
    - **`__call__`:**
        - 指定されたエピソード数だけ評価ループを実行します。
        - `env.reset()` で環境を初期化し、`policy()`（`ddpg.select_action`）で行動を選択し、`env.step()` で環境を進めます。
        - エピソード終了時に、`env.step()` から返される `info` ディクショナリを用いて、最終的な報酬、シャープレシオ、ソルティノレシオ、カルマーレシオ、最大ドローダウンなどの評価指標を収集します。
        - 全エピソードの評価指標の平均値を計算して返します。
    - **`save_results()`:**
        - 評価結果（総資産の推移グラフなど）をファイルに保存します。

#### `configs_stock.py`

- **役割:** プロジェクト全体で使用されるハイパーパラメータや定数を一元管理します。
- **主要な設定項目:**
    - `HMAX_NORMALIZE`: 1回の取引における最大株数。
    - `INITIAL_ACCOUNT_BALANCE`: 初期資金。
    - `STOCK_DIM`: 取引対象の銘柄数。
    - `N_DAYS`: 過去何日分のツイートを参照するか。
    - `MAX_TWEETS`: 1日に参照する最大ツイート数。
    - `TWEETS_EMB`: ツイートの埋め込み次元数。
    - `INPUT_TEXT`: 使用するテキスト情報の種類（`tweetonly`, `withSEC`など）。
    - `USING_MODEL`: 使用するモデルのファイル名（`model_2`, `model_4`など）。
    - `TRANSACTION_FEE_PERCENT`: 取引手数料。
    - `FEAT_DIMS`: 状態ベクトルの総次元数。入力の種類に応じて自動計算されます。

#### `memory.py`

- **役割:** DDPGのリプレイバッファを実装します。
- **主要なクラス:**
    - **`RingBuffer`:** 固定長のリングバッファ。古いデータから自動的に上書きされます。
    - **`SequentialMemory`:** `RingBuffer`を用いて、経験（状態、行動、報酬、次の状態、終了フラグ）を保存・サンプリングするクラス。DDPGの学習時に、過去の経験をランダムにミニバッチとして取り出すために使用されます。

#### `random_process.py`

- **役割:** エージェントの探索を促進するためのノイズを生成します。
- **主要なクラス:**
    - **`OrnsteinUhlenbeckProcess`:** 時間相関のあるノイズを生成するプロセス。物理的なシステムなど、連続的な行動空間を持つ強化学習タスクで、より効率的な探索を行うためにしばしば用いられます。

#### `util.py`

- **役割:** プロジェクト全体で使われる補助的な関数を提供します。
- **主要な関数:**
    - `to_numpy()`, `to_tensor()`: PyTorchのTensorとNumPyのndarrayを相互に変換します。
    - `soft_update()`, `hard_update()`: ターゲットネットワークの更新に使用します。
    - `get_output_folder()`: 結果を保存するための出力ディレクトリを生成します。
    - `prRed()`, `prGreen()` など: コンソール出力に色を付けるための関数です。

### 処理フローのまとめ

1.  **`main.py`** が実行され、引数を解析し、環境(`env.py`)とエージェント(`ddpg.py`)を初期化します。
2.  エージェントは `configs_stock.py` の `USING_MODEL` に基づいて、指定されたモデル（例: `model_2.py`）をロードします。
3.  **学習フェーズ:**
    a. エージェントは現在の状態(`state`)を観測します。
    b. `ddpg.select_action()` が `model.py` の `Actor` を使って行動を決定します。この際、探索のためにノイズが加えられます。
    c. `env.step()` がその行動を実行し、新しい状態と報酬を返します。
    d. この遷移（state, action, reward, next_state）が `memory.py` のリプレイバッファに保存されます。
    e. `ddpg.update_policy()` がリプレイバッファから経験をサンプリングし、`model.py` の `Actor` と `Critic` の重みを更新します。
    f. 一定間隔で `evaluator.py` が呼び出され、現在のエージェントの性能がテストデータで評価されます。最も性能の良いモデルが保存されます。
4.  **テストフェーズ:**
    a. 学習済みのモデルがロードされます。
    b. `evaluator.py` がテスト環境でエージェントを実行し、最終的なパフォーマンス指標（シャープレシオなど）を計算して出力します。

以上が `profit_naacl_v2` ディレクトリ内のコードの解析レポートです。
